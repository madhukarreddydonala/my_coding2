{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17a41ccf"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "GANs, or Generative Adversarial Networks, are a type of neural network architecture that allow neural networks to generate data. In the past few years, they’ve become one of the hottest subfields in deep learning, going from generating fuzzy images of digits to photorealistic images of faces.\n",
        "\n",
        "GANs learn a probability distribution of a dataset by pitting two neural networks against each other.\n",
        "- One model, the **generator**, tries to create images that look very similar to the dataset.\n",
        "- The other model, the **discriminator**, tries to detect whether the images generated were fake or not.\n",
        "\n",
        "<img style=\"float: center;\" src='images/gan1.png' width=500 height=400 caption=\"Feed Forward NN\">\n",
        "\n",
        "Training generative adversarial networks involve two objectives:\n",
        "1. The discriminator maximizes the probability of assigning the correct label to both training examples and images generated by the generator.\n",
        "2. The generator minimizes the probability that the discriminator can predict that what it generates is fake. i.e the generator becomes better at creating fakes.\n",
        "\n",
        "We'll be using the MNIST, a dataset of handwritten digits to implement GAN."
      ],
      "id": "17a41ccf"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yNnKOZuF8IV",
        "outputId": "3524decf-5e3c-42f1-a4ed-794c3bf57e7e"
      },
      "id": "3yNnKOZuF8IV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2cd83db8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Dropout, LeakyReLU\n",
        "from keras.models import Model, Sequential\n",
        "from keras.datasets import mnist\n",
        "# from keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers"
      ],
      "id": "2cd83db8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "423a63c6"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10) # reproduction\n",
        "\n",
        "# The dimension of our random noise vector.\n",
        "random_dim = 100"
      ],
      "id": "423a63c6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e54e4242"
      },
      "outputs": [],
      "source": [
        "def load_minst_data():\n",
        "    # load the data\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # normalize our inputs to be in the range[-1, 1]\n",
        "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
        "    # convert x_train with a shape of (60000, 28, 28) to (60000, 784)\n",
        "    # so we have 784 columns per row\n",
        "    x_train = x_train.reshape(60000, 784)\n",
        "    return (x_train, y_train, x_test, y_test)"
      ],
      "id": "e54e4242"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35f4088"
      },
      "source": [
        "The generator also needs random input vectors to generate images, and for this, we’ll be using numpy.\n",
        "\n",
        "**The GAN Function**\n",
        "\n",
        "The GAN plays a minimax game, where the entire network attempts to optimize the function V(D,G). This is the equation that defines what a GAN is doing:\n",
        "<img style=\"float: center;\" src='images/gan2.png' width=500 height=450 caption=\"Feed Forward NN\">\n",
        "\n",
        "Now to anyone who isn’t well versed in the math behind it, it looks terrifying, but the idea it represents is simple, yet powerful. It’s just a mathematical representation of the two objectives as defined above.\n",
        "\n",
        "The generator is defined by G(z), which converts some noise z we input into some data, like images.\n",
        "\n",
        "The discriminator is defined by D(x), which outputs the probability that the input x came from the real dataset or not.\n",
        "\n",
        "We want the predictions on the dataset by the discriminator to be as close to 1 as possible, and on the generator to be as close to 0 as possible. To achieve this, we use the log-likelihood of D(x) and 1-D(z) in the objective function. The log just makes sure that the closer it is to an incorrect value, the more it is penalized."
      ],
      "id": "d35f4088"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5e743888"
      },
      "outputs": [],
      "source": [
        "def get_optimizer():\n",
        "    return Adam(learning_rate=0.0002, beta_1=0.5)"
      ],
      "id": "5e743888"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generator(optimizer):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(256, input_dim=random_dim, \\\n",
        "            kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Dense(1024))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Dense(784, activation='tanh'))\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return generator"
      ],
      "metadata": {
        "id": "LcIFk4yHaEMl"
      },
      "id": "LcIFk4yHaEMl",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d93d84f"
      },
      "source": [
        "The generator is just a vanilla neural network model that takes a random input vector and outputs a 784-dim vector, which, when reshaped, becomes a 28*28 pixel image."
      ],
      "id": "5d93d84f"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d764f5a2"
      },
      "outputs": [],
      "source": [
        "def get_discriminator(optimizer):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(1024, input_dim=784, \\\n",
        "                kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "    discriminator.add(Dense(512))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return discriminator"
      ],
      "id": "d764f5a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e77b04f6"
      },
      "source": [
        "The discriminator is another neural network that takes the output of the previous network, a 784-dimensional vector, and outputs a probability between 0 and 1 that it came from the training dataset.\n",
        "\n",
        "**Compiling it into a GAN**"
      ],
      "id": "e77b04f6"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e6b944c9"
      },
      "outputs": [],
      "source": [
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "    # We initially set trainable to False since we only want to train either the\n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real or not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return gan"
      ],
      "id": "e6b944c9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9c9611a4"
      },
      "outputs": [],
      "source": [
        "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), \\\n",
        "                          figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', \\\n",
        "                   cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"
      ],
      "id": "9c9611a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537527b1"
      },
      "source": [
        "We now compile both models into a single adversarial network, setting the input as a 100-dimensional vector, and the output as the output of the discriminator."
      ],
      "id": "537527b1"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "91b95e0b"
      },
      "outputs": [],
      "source": [
        "def train(epochs=1, batch_size=128):\n",
        "\n",
        "    #1 Get the training and testing data\n",
        "    tf.config.run_functions_eagerly(True)\n",
        "    x_train, y_train, x_test, y_test = load_minst_data()\n",
        "    # Split the training data into batches of size 128\n",
        "    batch_count = x_train.shape[0] / batch_size\n",
        "\n",
        "    #2. Build our GAN netowrk\n",
        "    adam = get_optimizer()\n",
        "    generator = get_generator(adam)\n",
        "    discriminator = get_discriminator(adam)\n",
        "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
        "\n",
        "    # 3\n",
        "    for e in range(1, epochs+1):\n",
        "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in tqdm(range(int(batch_count))):\n",
        "            # 4. Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            image_batch = x_train[np.random.randint(0, x_train.shape[0], \\\n",
        "                                                    size=batch_size)]\n",
        "\n",
        "            # 5. Generate fake MNIST images\n",
        "            generated_images = generator.predict(noise)\n",
        "            X = np.concatenate([image_batch, generated_images])\n",
        "\n",
        "            # 6. Labels for generated and real data\n",
        "            y_dis = np.zeros(2*batch_size)\n",
        "            # One-sided label smoothing\n",
        "            y_dis[:batch_size] = 0.9\n",
        "\n",
        "            #7. Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "            #8. Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "        if e == 1 or e % 20 == 0:\n",
        "            plot_generated_images(e, generator)"
      ],
      "id": "91b95e0b"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "    # We initially set trainable to False since we only want to train either the\n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real or not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    # Recreate the optimizer instance for the GAN model\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=type(optimizer)(learning_rate=optimizer.learning_rate, beta_1=optimizer.beta_1)) # Create a new optimizer instance\n",
        "    return gan"
      ],
      "metadata": {
        "id": "MetqIc-DogYQ"
      },
      "id": "MetqIc-DogYQ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "    # We initially set trainable to False since we only want to train either the\n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real or not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    # Get learning_rate and beta_1 as float values\n",
        "    learning_rate = optimizer.learning_rate.numpy() # Get learning rate as a float\n",
        "    beta_1 = optimizer.beta_1.numpy() # Get beta_1 as a float\n",
        "    # Recreate the optimizer instance for the GAN model using float values\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=type(optimizer)(learning_rate=learning_rate, beta_1=beta_1)) # Create a new optimizer instance with float values\n",
        "    return gan"
      ],
      "metadata": {
        "id": "YU6lvnmMogLo"
      },
      "id": "YU6lvnmMogLo",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "a279cc90",
        "outputId": "245449f6-9b99-4704-dbf2-c626bfc09826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Argument `learning_rate` should be float, or an instance of LearningRateSchedule, or a callable (that takes in the current iteration value and returns the corresponding learning rate value). Received instead: learning_rate=<Variable path=adam/learning_rate, shape=(), dtype=float32, value=0.00019999999494757503>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f714215ffeb9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-ec6bab40e4ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gan_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9a4b7877a562>\u001b[0m in \u001b[0;36mget_gan_network\u001b[0;34m(discriminator, random_dim, generator, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Recreate the optimizer instance for the GAN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create a new optimizer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     ):\n\u001b[0;32m---> 62\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTFOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0;34m\"Argument `learning_rate` should be float, or an instance \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;34m\"of LearningRateSchedule, or a callable \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Argument `learning_rate` should be float, or an instance of LearningRateSchedule, or a callable (that takes in the current iteration value and returns the corresponding learning rate value). Received instead: learning_rate=<Variable path=adam/learning_rate, shape=(), dtype=float32, value=0.00019999999494757503>"
          ]
        }
      ],
      "source": [
        "train(20, 128)"
      ],
      "id": "a279cc90"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7acb7c4"
      },
      "source": [
        "1. First, we load the data and split the data into several batches to feed into our model\n",
        "2. Here we just initialize our GAN network based on the methods defined above\n",
        "3. This is our training loop, where we run for the specified number of epochs.\n",
        "4. We generate some random noise and take out some images from our dataset\n",
        "5. We generate some images using the generator and create a vector X that has some fake images and some real images\n",
        "6. We create a vector Y which has the “correct answers” that corresponds to X, with the fake images labeled 0 and the real images labeled 0.9. They’re labeled 0.9 instead of 1 because it helps the GAN train better, a method called one-sided label smoothing.\n",
        "7. We need to alternate the training between the discriminator and generator, so over here, we update the discriminator\n",
        "8. Finally, we update the discriminator."
      ],
      "id": "d7acb7c4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nrr47FNETaJZ"
      },
      "id": "Nrr47FNETaJZ",
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}